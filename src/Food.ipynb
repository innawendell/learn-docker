{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn import ensemble\n",
    "import boto3\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset comes from *Open Food Facts* and was downloaded from: https://www.kaggle.com/openfoodfacts/world-food-facts/home.\n",
    "'Open Food Facts is a free, open, collbarative database of food products from around the world, with ingredients, allergens, nutrition facts and all the tidbits of information we can find on product labels. Open Food Facts is a non-profit association of volunteers.' Over 5,000 contributors 'have added 100 000+ products from 150 countries using our Android, iPhone or Windows Phone app or their camera to scan barcodes and upload pictures of products and their labels.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA UPLOAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'io' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2e115d288104>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'innawendell'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'food.tsv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#food = pd.read_csv(r'/Users/admin/Documents/food.tsv', delimiter = '\\t', encoding='ISO-8859-1')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'io' is not defined"
     ]
    }
   ],
   "source": [
    "s3 = boto3.client('s3') \n",
    "\n",
    "obj = s3.get_object(Bucket='innawendell', Key='food.tsv') \n",
    "\n",
    "food = pd.read_csv(io.BytesIO(obj['Body'].read()), delimiter = '\\t', encoding='ISO-8859-1')\n",
    "#food = pd.read_csv(r'/Users/admin/Documents/food.tsv', delimiter = '\\t', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "food.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Retail Country: US"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our analysis, we will only focus on food that is on the market in the US."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "food['countries'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "food_abbr = food[food['countries'].isin(['US', 'United States', 'en:US'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_abbr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Ingredients and The Target - Energy Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_abbr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_energy = food_abbr.loc[:, ['ingredients_text', 'energy_100g']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_energy = features_energy.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify that all the nan values were excluded\n",
    "features_energy['ingredients_text'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_energy['energy_100g'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_energy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing The Ingredients Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making all word lower case\n",
    "lower = []\n",
    "for entry in features_energy['ingredients_text']:\n",
    "    lower.append(entry.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the fullstops from the strings\n",
    "\n",
    "no_fullstops = []\n",
    "for entry in lower:\n",
    "    entry = entry.replace('.', '')\n",
    "    no_fullstops.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting strings at commas\n",
    "split = []\n",
    "for entry in no_fullstops:\n",
    "    entry = entry.split(',')\n",
    "    split.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = []\n",
    "for entry in split:\n",
    "    for item in entry:\n",
    "        item = item.split(\"(\")\n",
    "        cleaned.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_new = []\n",
    "for entry in cleaned:\n",
    "    for item in entry:\n",
    "        item= item.strip(')')\n",
    "        cleaned_new.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_newer = []\n",
    "for entry in cleaned_new:\n",
    "        if entry.count('[') >0:\n",
    "            entry = entry.split('[') \n",
    "            cleaned_newer.append(entry)\n",
    "        else:\n",
    "            cleaned_newer.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_newer_one = []\n",
    "for entry in cleaned_newer:\n",
    "    if entry.count('and/or') >0:\n",
    "        entry = entry.split('and/or')\n",
    "        cleaned_newer_one.append(entry)\n",
    "    else:\n",
    "        cleaned_newer_one.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = {}\n",
    "for entry in cleaned_newer_one:\n",
    "    if type(entry) == list:\n",
    "        for word in entry:\n",
    "            if word.strip() not in unique_words:\n",
    "                unique_words[word.strip()] = 1\n",
    "            else:   \n",
    "                unique_words[word.strip()] += 1\n",
    "    else:\n",
    "        if entry.strip() not in unique_words:\n",
    "            unique_words[entry.strip()] = 1\n",
    "        else:\n",
    "            unique_words[entry.strip()] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent = {key:value for (key,value) in unique_words.items() if value >= 100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_vocab = frequent.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in frequent_vocab:\n",
    "    features_energy[word] = features_energy['ingredients_text'].str.contains(word, case=False, regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_energy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_energy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(features_energy.iloc[0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features_energy.iloc[:, 2:]\n",
    "y = features_energy['energy_100g']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=108)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge()\n",
    "fit = ridge.fit(X_train, y_train)\n",
    "y_pred = ridge.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_scores = []\n",
    "alphas = []\n",
    "\n",
    "for value in [1e-10, 1e-8, 1e-4, 1e-3, 1e-2, 1, 5, 10, 20]:\n",
    "    ridge = Ridge(alpha=value)\n",
    "    rmse = np.sqrt(np.mean(-cross_val_score(ridge, X, y, scoring = 'neg_mean_squared_error')))\n",
    "    rmse_scores.append(rmse)\n",
    "    alphas.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rmse_scores, columns = ['rmse'])\n",
    "df['alphas'] = alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha 20 is the winner\n",
    "\n",
    "df.sort_values(by=['rmse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_l = []\n",
    "rmses_l = []\n",
    "\n",
    "for value in [1e-15, 1e-10, 1e-8, 1e-5, 1e-4, 1e-3, 1e-2, 1, 5, 10]:\n",
    "    lasso = Lasso(alpha=value)\n",
    "    rmse_l = np.sqrt(np.mean(-cross_val_score(lasso, X, y, scoring = 'neg_mean_squared_error')))\n",
    "    rmses_l.append(rmse_l)\n",
    "    alphas_l.append(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "param_grid = {'alpha': [1e-4, 3e-4, 6e-4, 1e-3, 3e-3, 6e-3, 1e-2, 3e-2, 6e-2, 1e-21, 3e-1, 6e-1, 1, 3, 6], \n",
    "              'l1_ratio': 0.1, 0.5, 0.7, 0.9, 0.95, 0.99}\n",
    "\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "rmses_net = []\n",
    "parameters = []\n",
    "\n",
    "for params in grid:\n",
    "    net = ElasticNet(**params)\n",
    "    rmse_net = np.sqrt(np.mean(-cross_val_score(net, X, y, scoring = 'neg_mean_squared_error')))\n",
    "    rmses_net.append(rmse_net)\n",
    "    parameters.append(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [200, 300, 500, 700, 1000], \n",
    "              'max_depth': [3, 5, 10, 30, 50],\n",
    "             }\n",
    "              \n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "rmses_rfr = []\n",
    "parameters_rfr = []\n",
    "\n",
    "for params in grid:\n",
    "    rfr = RandomForestRegressor(**params)\n",
    "    rmses_rfr = np.sqrt(np.mean(-cross_val_score(rfr, X, y, scoring = 'neg_mean_squared_error')))\n",
    "    rmses_rfr.append(rmse_net)\n",
    "    parameters_rfr.append(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'loss': ['ls', 'lad', 'huber', 'quantile'], \n",
    "              'max_depth': [3, 5, 10, 30, 50],\n",
    "             'learning_rate': [0.5, 0.1, 0.05, 0.01]\n",
    "             'n_estimators'}\n",
    "             'subsample': [0.25, 0.5, 0.75, 1]\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "rmses_rfr = []\n",
    "parameters_rfr = []\n",
    "\n",
    "for params in grid:\n",
    "    gbr = GradientBoostingRegressor(**params)\n",
    "    rmses_rfr = np.sqrt(np.mean(-cross_val_score(rfr, X, y, scoring = 'neg_mean_squared_error')))\n",
    "    rmses_rfr.append(rmse_net)\n",
    "    parameters_rfr.append(params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
